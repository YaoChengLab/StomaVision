{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This script consists of code that evaluates high level objectives including:\n",
    "1. detection accuracy, F1 score\n",
    "2. measurement errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import helper\n",
    "import statistics as stat\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polygons_to_rotated_bboxes(polygons):\n",
    "    r\"\"\"\n",
    "    convert polygons to rotated bboxes using cv2.minAreaRect().\n",
    "\n",
    "    Args:\n",
    "     - polygons (list): is a list of polygon points [x1, y1, x2, y2,...]\n",
    "    \"\"\"\n",
    "    rbboxes = []\n",
    "    for p in polygons:\n",
    "        pts_x = p[::2]\n",
    "        pts_y = p[1::2]\n",
    "        pts = [[x, y] for x, y in zip(pts_x, pts_y)]\n",
    "        pts = np.array(pts, np.float32)\n",
    "        rect = cv2.minAreaRect(pts)  #  ((cx, cy), (w, h), a)\n",
    "        rbboxes.append(rect)\n",
    "    return rbboxes\n",
    "\n",
    "\n",
    "def check_bbox_intersection(rect1, rect2):\n",
    "    r\"\"\"\n",
    "    check if there exist an overlap between two bounding boxes.\n",
    "\n",
    "    Args:\n",
    "    --------------\n",
    "    - bbox_1: first bbox ((cx, cy), (w, h), a)\n",
    "    - bbox_2: second bbox ((cx, cy), (w, h), a)\n",
    "    \"\"\"\n",
    "    output = cv2.rotatedRectangleIntersection(rect1, rect2)\n",
    "\n",
    "    print(output)\n",
    "\n",
    "\n",
    "def convert_bbox_to_rbbox(bbox: list[float], angle=False):\n",
    "    r\"\"\"\n",
    "    convert bbox from format [x,y,w,h] to ((cx,cy), (w,h), a)\n",
    "    \"\"\"\n",
    "    if not angle:\n",
    "        return ((bbox[0], bbox[1]), (bbox[2], bbox[3]), 0)\n",
    "    else:\n",
    "        return ((bbox[0], bbox[1]), (bbox[2], bbox[3]), bbox[4])\n",
    "\n",
    "\n",
    "def convert_bboxes_to_rbboxes(bboxes: list[list], angle=False):\n",
    "    r\"\"\"\n",
    "    convert a list of bboxes to a list of rbboxes\n",
    "    \"\"\"\n",
    "    rbboxes = []\n",
    "    for bbox in bboxes:\n",
    "        rbboxes.append(convert_bbox_to_rbbox(bbox, angle))\n",
    "    return rbboxes\n",
    "\n",
    "\n",
    "def get_ground_truth_polygons(file_name: str) -> list:\n",
    "    lines = []\n",
    "    polygons = []\n",
    "    with open(file_name, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # prepare polygons and texts\n",
    "    for i, line in enumerate(lines):\n",
    "        chars = line.split(\" \")\n",
    "        chars = list(map(float, chars))\n",
    "        polygons.append(chars[1:])\n",
    "\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def scale_up_polygons(polygons: list, scale_h: int, scale_w: int):\n",
    "    for j, p in enumerate(polygons):\n",
    "        scales = [scale_h if i % 2 != 0 else scale_w for i in range(len(p))]\n",
    "        polygons[j] = [p[i] * scales[i] for i in range(len(p))]\n",
    "\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def scale_up_bboxes(bboxes: list, scale_h: int, scale_w: int):\n",
    "    output_bboxes = []\n",
    "\n",
    "    for b in bboxes:\n",
    "        cx = b[0][0] * scale_w  # scale x\n",
    "        cy = b[0][1] * scale_h  # scale y\n",
    "        w = b[1][0] * scale_w  # scale w\n",
    "        h = b[1][1] * scale_h  # scale h\n",
    "        a = b[2]\n",
    "        output_bboxes.append(((cx, cy), (w, h), a))\n",
    "\n",
    "    return output_bboxes\n",
    "\n",
    "\n",
    "def get_predict_bboxes(file_name: str) -> tuple[list, list]:\n",
    "    lines = []\n",
    "    bboxes = []\n",
    "    rbboxes = []\n",
    "    with open(file_name, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # prepare polygons and texts\n",
    "    is_rbboxes = False\n",
    "    for line in lines:\n",
    "        chars = line.split(\" \")\n",
    "        if chars[0] == \"rotated_bbox\\n\":\n",
    "            is_rbboxes = True\n",
    "            continue\n",
    "        chars = list(map(float, chars))\n",
    "        if not is_rbboxes:\n",
    "            bboxes.append(chars[1:])\n",
    "        else:\n",
    "            rbboxes.append(chars[1:])\n",
    "\n",
    "    bboxes = convert_bboxes_to_rbboxes(bboxes)\n",
    "    rbboxes = convert_bboxes_to_rbboxes(rbboxes, angle=True)\n",
    "    return bboxes, rbboxes\n",
    "\n",
    "\n",
    "def measurements_to_ratios(measurements: list):\n",
    "    ratios = []\n",
    "    for m in measurements:\n",
    "        r1 = m[0][0] / m[0][1]\n",
    "        r2 = m[1][0] / m[1][1]\n",
    "\n",
    "        # make sure the ratio is always between 0 and 1\n",
    "        if r1 > 1:\n",
    "            r1 = 1 / r1\n",
    "        if r2 > 1:\n",
    "            r2 = 1 / r2\n",
    "\n",
    "        r = (r1, r2)\n",
    "        ratios.append(r)\n",
    "    return ratios\n",
    "\n",
    "\n",
    "def get_error_metric(measurements: list):\n",
    "    ratios = measurements_to_ratios(measurements)\n",
    "\n",
    "    errs = []\n",
    "    for r in ratios:\n",
    "        errs.append(abs(r[0] - r[1]))\n",
    "\n",
    "    if len(errs) < 1:  # no measurement\n",
    "        err_mean = 0\n",
    "        err_median = 0\n",
    "    else:\n",
    "        err_mean = stat.mean(errs)\n",
    "        err_median = stat.median(errs)\n",
    "\n",
    "    if len(measurements) < 2:  # only one measurement\n",
    "        err_stdev = 0\n",
    "        err_var = 0\n",
    "    else:\n",
    "        err_stdev = stat.stdev(errs)\n",
    "        err_var = stat.variance(errs)\n",
    "\n",
    "    return (err_mean, err_stdev, err_var, err_median)\n",
    "\n",
    "\n",
    "def evaluate(img_filename, label_filename, pred_filename) -> dict:\n",
    "    r\"\"\"\n",
    "    This is the evaluation code that returns a dictionary consisting of:\n",
    "    (precision, recall, err_mean, err_stdev, err_var, err_median)\n",
    "    Args\n",
    "    -------------\n",
    "    - img_filename\n",
    "    - label_filename\n",
    "    - pred_filename\n",
    "    \"\"\"\n",
    "\n",
    "    # read image\n",
    "    im = cv2.imread(img_filename)\n",
    "    scale_h, scale_w = im.shape[0], im.shape[1]\n",
    "\n",
    "    # Get ground truth rbboxes\n",
    "    polygons_true = get_ground_truth_polygons(label_filename)\n",
    "    polygons_true = scale_up_polygons(polygons_true, scale_h, scale_w)\n",
    "    rbboxes_true = helper.fit_polygons_to_rotated_bboxes(polygons_true)\n",
    "\n",
    "    # Get predict bboxes (detection output) and rbboexes (from mask)\n",
    "    bboxes_pred, rbboxes_pred = get_predict_bboxes(pred_filename)\n",
    "    bboxes_pred = scale_up_bboxes(bboxes_pred, scale_h, scale_w)\n",
    "\n",
    "    # Now we have below three lists\n",
    "    #\n",
    "    # - masks_truth: ground truth masks converted into rbboxes\n",
    "    # - bboxes_pred: predict bboxes\n",
    "    # - masks_pred: predict masks converted into rbboxes\n",
    "    #\n",
    "    # We can then use cv2.rotatedRectangleIntersection(rect1, rect2) to see if check their pairwise intersections.\n",
    "    #\n",
    "    # test = cv2.rotatedRectangleIntersection(rect1, rect2)\n",
    "    # test[0] == 1 indicates rect1 and rect2 are intersect; other wise not intersect\n",
    "\n",
    "    total_stomata = 0\n",
    "    total_detected = 0\n",
    "    measurements = []  # measurement = ((h_true, w_true),(h_pred, w_pred))\n",
    "    for r_true in rbboxes_true:\n",
    "        total_stomata += 1\n",
    "        for b_pred in bboxes_pred:  # check if stomata is detected\n",
    "            test_detect = cv2.rotatedRectangleIntersection(r_true, b_pred)\n",
    "            if test_detect[0] == 1:  # stomata is detected in prediction\n",
    "                total_detected += 1\n",
    "                for (\n",
    "                    r_pred\n",
    "                ) in rbboxes_pred:  # check if the predicted mask exists for the stomata\n",
    "                    test_mask = cv2.rotatedRectangleIntersection(b_pred, r_pred)\n",
    "                    if test_mask[0] == 1:  # find the corresponding mask\n",
    "                        # compute height and weight\n",
    "                        m = ((r_true[1][0], r_true[1][1]), (r_pred[1][0], r_pred[1][1]))\n",
    "                        measurements.append(m)\n",
    "                        break\n",
    "                break\n",
    "\n",
    "    precision = total_detected / len(bboxes_pred)\n",
    "    recall = total_detected / total_stomata\n",
    "    err_mean, err_stdev, err_var, err_median = get_error_metric(measurements)\n",
    "\n",
    "    result = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"err_mean\": err_mean,\n",
    "        \"err_stdev\": err_stdev,\n",
    "        \"err_var\": err_var,\n",
    "        \"err_median\": err_median,\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def inference_to_dataframe(img_filename, pred_filename) -> tuple[str, pd.DataFrame]:\n",
    "    r\"\"\"tp\n",
    "    This is the inferencecode that convert model inference outputs to\n",
    "    Args\n",
    "    -------------\n",
    "    - img_filename\n",
    "    - label_filename\n",
    "    - pred_filename\n",
    "    \"\"\"\n",
    "    img_name = os.path.splitext(img_filename)[0].split(\"/\")[-1]\n",
    "\n",
    "    # read image\n",
    "    im = cv2.imread(img_filename)\n",
    "    scale_h, scale_w = im.shape[0], im.shape[1]\n",
    "\n",
    "    # Get predict bounding bboxes\n",
    "    bboxes_pred, rbboxes_pred = get_predict_bboxes(pred_filename)\n",
    "    bboxes_pred = scale_up_bboxes(bboxes_pred, scale_h, scale_w)\n",
    "\n",
    "    measurements = []  # measurement = (id, long_axis, short_axis, ratio)\n",
    "\n",
    "    for idx, r_pred in enumerate(rbboxes_pred):\n",
    "        # Get long and short axis\n",
    "        if r_pred[1][0] > r_pred[1][1]:\n",
    "            long_axis, short_axis = r_pred[1][0], r_pred[1][1]\n",
    "        else:\n",
    "            long_axis, short_axis = r_pred[1][1], r_pred[1][0]\n",
    "\n",
    "        ratio = short_axis / long_axis\n",
    "        measurements.append((idx, scale_h, scale_w, long_axis, short_axis, ratio))\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        measurements,\n",
    "        columns=[\"id\", \"img_height\", \"img_width\", \"long_axis\", \"short_axis\", \"ratio\"],\n",
    "    )\n",
    "\n",
    "    return img_name, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../../google-drive/stomaVDP/2023-all-337\"\n",
    "label_dir = os.path.join(data_dir, \"labels\", \"train\")\n",
    "img_dir = os.path.join(data_dir, \"images\", \"train\")\n",
    "\n",
    "img_file = \"CK005.jpg\"\n",
    "img_filename = os.path.join(img_dir, img_file)\n",
    "true_file = \"CK005.txt\"\n",
    "true_filename = os.path.join(label_dir, true_file)\n",
    "\n",
    "pred_filename = \"../runs/predict-seg/exp45/labels/CK005.txt\"\n",
    "\n",
    "img_name, df = inference_to_dataframe(img_filename, pred_filename)\n",
    "print(img_name)\n",
    "df[\"img_name\"] = img_name\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 inference files matches with images...\n",
      "69 images to process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:02, 27.40it/s]\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"/mnt/linux/abrc/abrc/dataset/stomaVDP/2023-all-new-337/images/val\"\n",
    "pred_dir = \"../runs/predict-seg/exp2/labels\"\n",
    "output_file = \"../runs/predict-seg/exp2/labels.csv\"\n",
    "\n",
    "pred_files = os.listdir(pred_dir)\n",
    "img_files = os.listdir(img_dir)\n",
    "\n",
    "imgs = []\n",
    "for f in pred_files:\n",
    "    filename = os.path.splitext(f)[0]\n",
    "    imgs.append([img for img in img_files if filename in img][0])\n",
    "\n",
    "\n",
    "print(f\"{len(pred_files)} inference files matches with images...\")\n",
    "# print(pred_files)\n",
    "print(f\"{len(imgs)} images to process...\")\n",
    "# print(imgs)\n",
    "\n",
    "result_dfs = []\n",
    "for pred, img in tqdm(zip(pred_files, imgs)):\n",
    "    img_filename = os.path.join(img_dir, img)\n",
    "    pred_filename = os.path.join(pred_dir, pred)\n",
    "\n",
    "    filename, df = inference_to_dataframe(img_filename, pred_filename)\n",
    "    df[\"img_name\"] = filename\n",
    "    result_dfs.append(df)\n",
    "\n",
    "\n",
    "df_output = pd.concat(result_dfs, axis=0, ignore_index=True)\n",
    "df_output.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Main function for one file (cell for development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only used for code development.\n",
    "# Use next one indead.\n",
    "\n",
    "data_dir = \"../../../google-drive/stomaVDP/2023-all-337\"\n",
    "label_dir = os.path.join(data_dir, \"labels\", \"train\")\n",
    "img_dir = os.path.join(data_dir, \"images\", \"train\")\n",
    "\n",
    "img_file = \"CK005.jpg\"\n",
    "img_file_name = os.path.join(img_dir, img_file)\n",
    "ground_truth_file = \"CK005.txt\"\n",
    "file_name = os.path.join(label_dir, ground_truth_file)\n",
    "\n",
    "# Read image\n",
    "im = cv2.imread(img_file_name)\n",
    "scale_h, scale_w = im.shape[0], im.shape[1]\n",
    "\n",
    "# Get ground-truth bboxes\n",
    "polygons_true = get_ground_truth_polygons(file_name)\n",
    "polygons_true = scale_up_polygons(polygons_true, scale_h, scale_w)\n",
    "bboxes_true = helper.fit_polygons_to_rotated_bboxes(polygons_true)\n",
    "\n",
    "# Get predict bboxes\n",
    "predict_file = \"../runs/predict-seg/exp45/labels/CK005.txt\"\n",
    "bboxes_pred, rbboxes_pred = get_predict_bboxes(predict_file)\n",
    "bboxes_pred = scale_up_bboxes(bboxes_pred, scale_h, scale_w)\n",
    "\n",
    "# Now we have below three lists\n",
    "#\n",
    "# - masks_truth: ground truth masks converted into rbboxes\n",
    "# - bboxes_pred: predict bboxes\n",
    "# - masks_pred: predict masks converted into rbboxes\n",
    "#\n",
    "# We can then use cv2.rotatedRectangleIntersection(rect1, rect2) to see if check their pairwise intersections.\n",
    "#\n",
    "# test = cv2.rotatedRectangleIntersection(rect1, rect2)\n",
    "# test[0] == 1 indicates rect1 and rect2 are intersect; other wise not intersect\n",
    "\n",
    "total_stomata = 0\n",
    "total_detected = 0\n",
    "measurements = []  # measurement = ((h_true, w_true),(h_pred, w_pred))\n",
    "for b_true in bboxes_true:\n",
    "    total_stomata += 1\n",
    "    for b_pred in bboxes_pred:  # check if stomata is detected\n",
    "        test_detect = cv2.rotatedRectangleIntersection(b_true, b_pred)\n",
    "        if test_detect[0] == 1:  # stomata is detected in prediction\n",
    "            total_detected += 1\n",
    "            for (\n",
    "                r_pred\n",
    "            ) in rbboxes_pred:  # check if the predicted mask exists for the stomata\n",
    "                test_mask = cv2.rotatedRectangleIntersection(b_pred, r_pred)\n",
    "                if test_mask[0] == 1:  # find the corresponding mask\n",
    "                    # compute height and weight\n",
    "                    m = ((b_true[1][0], b_true[1][1]), (r_pred[1][0], r_pred[1][1]))\n",
    "                    measurements.append(m)\n",
    "                    break\n",
    "            break\n",
    "\n",
    "precision = total_detected / len(bboxes_pred)\n",
    "recall = total_detected / total_stomata\n",
    "err_mean, err_stdev, _, err_median = get_error_metric(measurements)\n",
    "print(\"==== exp results ====\")\n",
    "print(f\"precision: {precision}, recall: {recall}\")\n",
    "print(measurements)\n",
    "print(f\"err_mean: {err_mean}, err_stdev: {err_stdev}, err_median: {err_median}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function that iterate through validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "69\n",
      "['10090.txt', 'u_tr_242.txt', '0615_08-04.3.5.txt', '1.txt', 'c_tr_91_cut_want.txt', '0615_07-10.2.1.txt', 'c_tr_97_cut_want.txt', '5.txt', '0615_03-14.2.2.txt', '34.txt', '0611_05-12.3.5.txt', 'u_tr_160.txt', '0615_03-05.2.2.txt', 'u_tr_95.txt', 'BE_20X007.txt', '0611_09-14.2.2.txt', 'u_tr_279.txt', 'u_tr_39.txt', 'c_tr_24_cut_want.txt', 'p_tr_102.txt', 'u_tr_299.txt', '0615_03-02.1.1.txt', '0611_12-11.3.2.txt', '0611_10-6.2.5.txt', 'u_tr_209.txt', 'p_tr_30.txt', '0611_12-03.3.4.txt', 'BE_20X002.txt', '197.txt', 'c_tr_41_cut_want.txt', '0615_09-8.3.4.txt', 'C1L006.txt', 'BE_20X015.txt', '0611_16-14.2.2.txt', '150.txt', 'p_tr_98.txt', 'u_tr_245.txt', '10394.txt', '0615_12-13.3.2.txt', 'u_tr_113.txt', '0615_03-13.1.3.txt', '0611_07-08.2.2.txt', 'C1L002.txt', '0615_10-02.1.1.txt', 'p_tr_53.txt', 'u_tr_149.txt', 'u_tr_55.txt', 'p_tr_36.txt', 'p_tr_10.txt', 'p_tr_112.txt', 'u_tr_212.txt', '0611_07-01.3.3.txt', '10356.txt', '190.txt', 'c_tr_5_cut_want.txt', 'p_tr_113.txt', 'BE_20X008.txt', '10181.txt', 'p_tr_66.txt', 'CK007.txt', 'u_tr_294.txt', 'c_tr_154_cut_want.txt', '0611_15-9.6.5.txt', '0615_05-15.2.1.txt', 'p_tr_16.txt', '9999.txt', '10306.txt', 'p_tr_52.txt', '0611_02-12.2.2.txt']\n",
      "['10090.jpg', 'u_tr_242.png', '0615_08-04.3.5.tif', '10090.jpg', 'c_tr_91_cut_want.jpg', '0615_07-10.2.1.tif', 'c_tr_97_cut_want.jpg', '0615_03-13.1.3.tif', '0615_03-14.2.2.tif', '34.jpg', '0611_05-12.3.5.tif', 'u_tr_160.png', '0615_03-05.2.2.tif', 'u_tr_95.png', 'BE_20X007.jpg', '0611_09-14.2.2.tif', 'u_tr_279.png', 'u_tr_39.png', 'c_tr_24_cut_want.jpg', 'p_tr_102.png', 'u_tr_299.png', '0615_03-02.1.1.tif', '0611_12-11.3.2.tif', '0611_10-6.2.5.tif', 'u_tr_209.png', 'p_tr_30.png', '0611_12-03.3.4.tif', 'BE_20X002.jpg', '197.jpg', 'c_tr_41_cut_want.jpg', '0615_09-8.3.4.tif', 'C1L006.jpg', 'BE_20X015.jpg', '0611_16-14.2.2.tif', '150.jpg', 'p_tr_98.png', 'u_tr_245.png', '10394.jpg', '0615_12-13.3.2.tif', 'u_tr_113.png', '0615_03-13.1.3.tif', '0611_07-08.2.2.tif', 'C1L002.jpg', '0615_10-02.1.1.tif', 'p_tr_53.png', 'u_tr_149.png', 'u_tr_55.png', 'p_tr_36.png', 'p_tr_102.png', 'p_tr_112.png', 'u_tr_212.png', '0611_07-01.3.3.tif', '10356.jpg', '190.jpg', 'c_tr_5_cut_want.jpg', 'p_tr_113.png', 'BE_20X008.jpg', '10181.jpg', 'p_tr_66.png', 'CK007.jpg', 'u_tr_294.png', 'c_tr_154_cut_want.jpg', '0611_15-9.6.5.tif', '0615_05-15.2.1.tif', 'p_tr_16.png', '9999.jpg', '10306.jpg', 'p_tr_52.png', '0611_02-12.2.2.tif']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:03, 21.84it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/mnt/linux/abrc/abrc/dataset/stomaVDP/2023-all-new-337\"\n",
    "label_dir = os.path.join(data_dir, \"labels\", \"val\")\n",
    "img_dir = os.path.join(data_dir, \"images\", \"val\")\n",
    "\n",
    "pred_dir = \"../runs/predict-seg/exp2/labels\"\n",
    "output_file = \"../runs/predict-seg/exp2/results.json\"\n",
    "\n",
    "\n",
    "label_files = os.listdir(label_dir)\n",
    "pred_files = os.listdir(pred_dir)\n",
    "img_files = os.listdir(img_dir)\n",
    "\n",
    "inter_files = list(set(label_files) & set(pred_files))\n",
    "\n",
    "imgs = []\n",
    "for f in inter_files:\n",
    "    filename = os.path.splitext(f)[0]\n",
    "    imgs.append([img for img in img_files if filename in img][0])\n",
    "\n",
    "\n",
    "print(len(inter_files))\n",
    "print(len(imgs))\n",
    "print(inter_files)\n",
    "print(imgs)\n",
    "results = []\n",
    "for label, img in tqdm(zip(inter_files, imgs)):\n",
    "    img_filename = os.path.join(img_dir, img)\n",
    "    label_filename = os.path.join(label_dir, label)\n",
    "    pred_filename = os.path.join(pred_dir, label)\n",
    "\n",
    "    result = evaluate(img_filename, label_filename, pred_filename)\n",
    "    result[\"name\"] = os.path.splitext(img)[0]\n",
    "    results.append(result)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_file = \"../runs/predict-seg/exp2/results.json\"\n",
    "\n",
    "df = pd.read_json(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"name\")\n",
    "df.to_csv(\"../runs/predict-seg/exp2/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abrc-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
